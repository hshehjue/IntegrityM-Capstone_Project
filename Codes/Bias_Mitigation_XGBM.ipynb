{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9df8a8d9",
   "metadata": {},
   "source": [
    "# Disparate Impact by Providers' Gender \n",
    "## the best model: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ab2716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import copy\n",
    "from collections import Counter\n",
    "from numpy import where\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import itertools\n",
    "from interpret.glassbox import ExplainableBoostingClassifier \n",
    "import xgboost as xgb\n",
    "from interpret.perf import ROC  \n",
    "from imblearn import over_sampling\n",
    "from imblearn import under_sampling\n",
    "from imblearn.pipeline import Pipeline\n",
    "import os              # for directory and file manipulation\n",
    "import numpy as np     # for basic array manipulation\n",
    "import pandas as pd    # for dataframe manipulation\n",
    "import datetime        # for timestamp\n",
    "\n",
    "# for model eval\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, mean_squared_error, roc_auc_score\n",
    "\n",
    "# global constants \n",
    "ROUND = 3    \n",
    "\n",
    "# set global random seed for better reproducibility\n",
    "SEED = 1234\n",
    "seed = 1234\n",
    "NTHREAD = 4\n",
    "\n",
    "#import sagemaker, boto3, os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc995784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the cleaned dataset containing Gender feature\n",
    "\n",
    "#%cd /Users/alex/Desktop/Master/BA_Practicum_6217_10/Project/dataset\n",
    "partB = pd.read_csv(\"partB_new5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cba3f55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 348887 entries, 0 to 348886\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   NPI                 348887 non-null  int64  \n",
      " 1   Gender              348887 non-null  object \n",
      " 2   Type                348887 non-null  object \n",
      " 3   Place_Of_Srvc       348887 non-null  object \n",
      " 4   Tot_Benes           348887 non-null  int64  \n",
      " 5   Tot_Srvcs           348887 non-null  float64\n",
      " 6   Tot_Bene_Day_Srvcs  348887 non-null  int64  \n",
      " 7   Avg_Sbmtd_Chrg      348887 non-null  float64\n",
      " 8   Avg_Mdcr_Alowd_Amt  348887 non-null  float64\n",
      " 9   Avg_Mdcr_Pymt_Amt   348887 non-null  float64\n",
      " 10  Avg_Mdcr_Stdzd_Amt  348887 non-null  float64\n",
      " 11  Fraud               348887 non-null  int64  \n",
      "dtypes: float64(5), int64(4), object(3)\n",
      "memory usage: 31.9+ MB\n"
     ]
    }
   ],
   "source": [
    "partB.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a4fa3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding \n",
    "\n",
    "# Convert the Fraud variable to object datatype\n",
    "partB[\"Fraud\"] = partB[\"Fraud\"].astype(object)\n",
    "\n",
    "# Encoding\n",
    "encoded_partB = pd.get_dummies(partB, drop_first = True)\n",
    "\n",
    "# Rename some of the changed variable names\n",
    "encoded_partB.rename(columns = {\"Gender_M\":\"Gender\", \"Fraud_1\":\"Fraud\", \"Place_Of_Srvc_O\":\"Place_Of_Srvc\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eeb96e",
   "metadata": {},
   "source": [
    "## Data Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b907c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign X and y features\n",
    "\n",
    "X_var = list(encoded_partB.columns)\n",
    "\n",
    "for var in [\"NPI\",\"Fraud\"]:\n",
    "    X_var.remove(var)\n",
    "\n",
    "y_var = \"Fraud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8ed21e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the whole dataset into train and test dataset\n",
    "# Using a stratified random sampling so that the Fraud-class (1) data are evenly split into train & test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(encoded_partB[X_var], \n",
    "                                                    encoded_partB[y_var], \n",
    "                                                    test_size=0.2, \n",
    "                                                    stratify=encoded_partB[\"Fraud\"])\n",
    "\n",
    "# Also concatenate the split x & y dataframes \n",
    "tr_df = pd.concat([x_train, y_train], axis = 1)\n",
    "te_df = pd.concat([x_test, y_test], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5711f8",
   "metadata": {},
   "source": [
    "## Over-Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6595ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE the dataset\n",
    "oversample = over_sampling.SMOTE()\n",
    "tr_X, tr_y = oversample.fit_resample(tr_df[X_var], tr_df[y_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd090bf",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f8f8c",
   "metadata": {},
   "source": [
    "### Data Partitioning (Train & Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd64fd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data rows = 355544, columns = 117\n",
      "Validation data rows = 151926, columns = 117\n"
     ]
    }
   ],
   "source": [
    "trans_tr_df = pd.concat([tr_X, tr_y], axis = 1)\n",
    "\n",
    "# Split train and validation sets \n",
    "np.random.seed(SEED)\n",
    "\n",
    "ratio = 0.7 # split train & validation sets with 7:3 ratio \n",
    "\n",
    "split = np.random.rand(len(trans_tr_df)) < ratio # define indices of 70% corresponding to the training set\n",
    "\n",
    "train = trans_tr_df[split]\n",
    "valid = trans_tr_df[~split]\n",
    "\n",
    "# summarize split\n",
    "print('Train data rows = %d, columns = %d' % (train.shape[0], train.shape[1]))\n",
    "print('Validation data rows = %d, columns = %d' % (valid.shape[0], valid.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb7050f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassign X_var\n",
    "X_var.remove(\"Gender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a412ea87",
   "metadata": {},
   "source": [
    "### XGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fd97d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_grid(dtrain, dvalid, mono_constraints=None, gs_params=None, n_models=None,\n",
    "             ntree=None, early_stopping_rounds=None, verbose=False, seed=None):\n",
    "    \n",
    "    \"\"\" Performs a random grid search over n_models and gs_params.\n",
    "\n",
    "    :param dtrain: Training data in LightSVM format.\n",
    "    :param dvalid: Validation data in LightSVM format.\n",
    "    :param mono_constraints: User-supplied monotonicity constraints.\n",
    "    :param gs_params: Dictionary of lists of potential XGBoost parameters over which to search.\n",
    "    :param n_models: Number of random models to evaluate.\n",
    "    :param ntree: Number of trees in XGBoost model.\n",
    "    :param early_stopping_rounds: XGBoost early stopping rounds.\n",
    "    :param verbose: Whether to display training iterations, default False.\n",
    "    :param seed: Random seed for better interpretability.\n",
    "    :return: Best candidate model from random grid search.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # cartesian product of gs_params\n",
    "    keys, values = zip(*gs_params.items())\n",
    "    experiments = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    # preserve exact reproducibility for this function\n",
    "    np.random.seed(SEED) \n",
    "    \n",
    "    # select randomly from cartesian product space\n",
    "    selected_experiments = np.random.choice(len(experiments), n_models)\n",
    "\n",
    "    # set global params for objective,  etc.\n",
    "    params = {'booster': 'gbtree',\n",
    "              'eval_metric': 'auc',\n",
    "              'nthread': NTHREAD,\n",
    "              'objective': 'binary:logistic',\n",
    "              'seed': SEED}\n",
    "\n",
    "    # init grid search loop\n",
    "    best_candidate = None\n",
    "    best_score = 0\n",
    "\n",
    "    # grid search loop\n",
    "    for i, exp in enumerate(selected_experiments):\n",
    "\n",
    "        params.update(experiments[exp])  # override global params with current grid run params\n",
    "\n",
    "        print('Grid search run %d/%d:' % (int(i + 1), int(n_models)))\n",
    "        print('Training with parameters:', params)\n",
    "\n",
    "        # train on current params\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "        \n",
    "        if mono_constraints is not None:\n",
    "            params['monotone_constraints'] = mono_constraints\n",
    "        \n",
    "        candidate = xgb.train(params,\n",
    "                              dtrain,\n",
    "                              ntree,\n",
    "                              early_stopping_rounds=early_stopping_rounds,\n",
    "                              evals=watchlist,\n",
    "                              verbose_eval=verbose)    \n",
    "\n",
    "        # determine if current model is better than previous best\n",
    "        if candidate.best_score > best_score:\n",
    "            best_candidate = candidate\n",
    "            best_score = candidate.best_score\n",
    "            print('Grid search new best score discovered at iteration %d/%d: %.4f.' %\n",
    "                             (int(i + 1), int(n_models), candidate.best_score))\n",
    "\n",
    "        print('---------- ----------')\n",
    "            \n",
    "    return best_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "224ac115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search run 1/1:\n",
      "Training with parameters: {'booster': 'gbtree', 'eval_metric': 'auc', 'nthread': 4, 'objective': 'binary:logistic', 'seed': 1234, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.9, 'eta': 0.5, 'max_depth': 7, 'reg_alpha': 0.005, 'reg_lambda': 0.005, 'subsample': 0.9, 'min_child_weight': 1, 'gamma': 0.2}\n",
      "Grid search new best score discovered at iteration 1/1: 0.9840.\n",
      "---------- ----------\n"
     ]
    }
   ],
   "source": [
    "gs_params = {'colsample_bytree': [0.7],\n",
    "             'colsample_bylevel': [0.9],\n",
    "             'eta': [0.5],\n",
    "             'max_depth': [7], \n",
    "             'reg_alpha': [0.005],\n",
    "             'reg_lambda': [0.005],\n",
    "             'subsample': [0.9],\n",
    "             'min_child_weight': [1], \n",
    "             'gamma': [0.2]}\n",
    "\n",
    "# Convert data to SVMLight format\n",
    "dtrain = xgb.DMatrix(train[X_var], train[y_var])\n",
    "dvalid = xgb.DMatrix(valid[X_var], valid[y_var])\n",
    "\n",
    "best_mxgb = xgb_grid(dtrain, dvalid, gs_params=gs_params, n_models=1, ntree=1000, early_stopping_rounds=100, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5901b50a",
   "metadata": {},
   "source": [
    "### Combine valid set with the best prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75fedc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tot_Benes</th>\n",
       "      <th>Tot_Srvcs</th>\n",
       "      <th>Tot_Bene_Day_Srvcs</th>\n",
       "      <th>Avg_Sbmtd_Chrg</th>\n",
       "      <th>Avg_Mdcr_Alowd_Amt</th>\n",
       "      <th>Avg_Mdcr_Pymt_Amt</th>\n",
       "      <th>Avg_Mdcr_Stdzd_Amt</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Type_Advanced Heart Failure and Transplant Cardiology</th>\n",
       "      <th>Type_Allergy/ Immunology</th>\n",
       "      <th>...</th>\n",
       "      <th>Type_Thoracic Surgery</th>\n",
       "      <th>Type_Undefined Physician type</th>\n",
       "      <th>Type_Undersea and Hyperbaric Medicine</th>\n",
       "      <th>Type_Unknown Physician Specialty Code</th>\n",
       "      <th>Type_Unknown Supplier/Provider</th>\n",
       "      <th>Type_Urology</th>\n",
       "      <th>Type_Vascular Surgery</th>\n",
       "      <th>Place_Of_Srvc</th>\n",
       "      <th>Fraud</th>\n",
       "      <th>phat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>11.690000</td>\n",
       "      <td>11.690000</td>\n",
       "      <td>16.030000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.269147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>163.0</td>\n",
       "      <td>163</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>11.620184</td>\n",
       "      <td>11.620184</td>\n",
       "      <td>11.620184</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.566231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>159</td>\n",
       "      <td>159.0</td>\n",
       "      <td>159</td>\n",
       "      <td>1665.635220</td>\n",
       "      <td>219.603396</td>\n",
       "      <td>174.243208</td>\n",
       "      <td>176.952201</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>185</td>\n",
       "      <td>193.0</td>\n",
       "      <td>193</td>\n",
       "      <td>174.147668</td>\n",
       "      <td>50.593990</td>\n",
       "      <td>38.662591</td>\n",
       "      <td>39.153990</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111</td>\n",
       "      <td>175.0</td>\n",
       "      <td>175</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>140.234686</td>\n",
       "      <td>105.528400</td>\n",
       "      <td>109.437200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.074380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tot_Benes  Tot_Srvcs  Tot_Bene_Day_Srvcs  Avg_Sbmtd_Chrg  \\\n",
       "0         11       12.0                  12       20.000000   \n",
       "1        100      163.0                 163       33.000000   \n",
       "2        159      159.0                 159     1665.635220   \n",
       "3        185      193.0                 193      174.147668   \n",
       "4        111      175.0                 175      231.000000   \n",
       "\n",
       "   Avg_Mdcr_Alowd_Amt  Avg_Mdcr_Pymt_Amt  Avg_Mdcr_Stdzd_Amt  Gender  \\\n",
       "0           11.690000          11.690000           16.030000       0   \n",
       "1           11.620184          11.620184           11.620184       0   \n",
       "2          219.603396         174.243208          176.952201       1   \n",
       "3           50.593990          38.662591           39.153990       1   \n",
       "4          140.234686         105.528400          109.437200       0   \n",
       "\n",
       "   Type_Advanced Heart Failure and Transplant Cardiology  \\\n",
       "0                                                  0       \n",
       "1                                                  0       \n",
       "2                                                  0       \n",
       "3                                                  0       \n",
       "4                                                  0       \n",
       "\n",
       "   Type_Allergy/ Immunology  ...  Type_Thoracic Surgery  \\\n",
       "0                         0  ...                      0   \n",
       "1                         0  ...                      0   \n",
       "2                         0  ...                      0   \n",
       "3                         0  ...                      0   \n",
       "4                         0  ...                      0   \n",
       "\n",
       "   Type_Undefined Physician type  Type_Undersea and Hyperbaric Medicine  \\\n",
       "0                              0                                      0   \n",
       "1                              0                                      0   \n",
       "2                              0                                      0   \n",
       "3                              0                                      0   \n",
       "4                              0                                      0   \n",
       "\n",
       "   Type_Unknown Physician Specialty Code  Type_Unknown Supplier/Provider  \\\n",
       "0                                      0                               0   \n",
       "1                                      0                               0   \n",
       "2                                      0                               0   \n",
       "3                                      0                               0   \n",
       "4                                      0                               0   \n",
       "\n",
       "   Type_Urology  Type_Vascular Surgery  Place_Of_Srvc  Fraud      phat  \n",
       "0             0                      0              1      0  0.269147  \n",
       "1             0                      0              1      0  0.566231  \n",
       "2             0                      0              0      0  0.001677  \n",
       "3             0                      0              0      0  0.018855  \n",
       "4             0                      0              1      0  0.074380  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtest = xgb.DMatrix(te_df[X_var])\n",
    "best_mxgb_phat = pd.DataFrame(best_mxgb.predict(dtest, iteration_range=(0, best_mxgb.best_ntree_limit)), columns=['phat'])\n",
    "best_mxgb_phat = pd.concat([te_df.reset_index(drop=True), best_mxgb_phat], axis=1)\n",
    "best_mxgb_phat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0a46f",
   "metadata": {},
   "source": [
    "## Mitigating Discrimination "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b68af0",
   "metadata": {},
   "source": [
    "### Utility functions \n",
    "### Calculate confusion matrices by demographic group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a910419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(frame, y, yhat, by=None, level=None, cutoff=0.2, verbose=True):\n",
    "\n",
    "    \"\"\" Creates confusion matrix from pandas dataframe of y and yhat values, can be sliced \n",
    "        by a variable and level.\n",
    "    \n",
    "        :param frame: Pandas dataframe of actual (y) and predicted (yhat) values.\n",
    "        :param y: Name of actual value column.\n",
    "        :param yhat: Name of predicted value column.\n",
    "        :param by: By variable to slice frame before creating confusion matrix, default None.\n",
    "        :param level: Value of by variable to slice frame before creating confusion matrix, default None.\n",
    "        :param cutoff: Cutoff threshold for confusion matrix, default 0.5. \n",
    "        :param verbose: Whether to print confusion matrix titles, default True. \n",
    "        :return: Confusion matrix as pandas dataframe. \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # determine levels of target (y) variable\n",
    "    # sort for consistency\n",
    "    level_list = list(frame[y].unique())\n",
    "    level_list.sort(reverse=True) \n",
    "\n",
    "    # init confusion matrix\n",
    "    cm_frame = pd.DataFrame(columns=['actual: ' +  str(i) for i in level_list], \n",
    "                            index=['predicted: ' + str(i) for i in level_list])\n",
    "    \n",
    "    # don't destroy original data\n",
    "    frame_ = frame.copy(deep=True)\n",
    "    \n",
    "    # convert numeric predictions to binary decisions using cutoff\n",
    "    dname = 'd_' + str(y)\n",
    "    frame_[dname] = np.where(frame_[yhat] > cutoff , 1, 0)\n",
    "    \n",
    "    # slice frame\n",
    "    if (by is not None) & (level is not None):\n",
    "        frame_ = frame_[frame[by] == level]\n",
    "    \n",
    "    # calculate size of each confusion matrix value\n",
    "    for i, lev_i in enumerate(level_list):\n",
    "        for j, lev_j in enumerate(level_list):\n",
    "            cm_frame.iat[j, i] = frame_[(frame_[y] == lev_i) & (frame_[dname] == lev_j)].shape[0]\n",
    "            # i, j vs. j, i nasty little bug ... updated 8/30/19\n",
    "    \n",
    "    # output results\n",
    "    if verbose:\n",
    "        if by is None:\n",
    "            print('Confusion matrix:')\n",
    "        else:\n",
    "            print('Confusion matrix by ' + by + '=' + str(level))\n",
    "    \n",
    "    return cm_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e88b1",
   "metadata": {},
   "source": [
    "### Calculate Adverse Impact Ratio (AIR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ab789c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def air(cm_dict, reference_key, protected_key, verbose=True):\n",
    "\n",
    "    \"\"\" Calculates the adverse impact ratio as a quotient between protected and \n",
    "        reference group acceptance rates: protected_prop/reference_prop. \n",
    "        Optionally prints intermediate values. ASSUMES 0 IS \"POSITIVE\" OUTCOME!\n",
    "\n",
    "        :param cm_dict: Dictionary of demographic group confusion matrices. \n",
    "        :param reference_key: Name of reference group in cm_dict as a string.\n",
    "        :param protected_key: Name of protected group in cm_dict as a string.\n",
    "        :param verbose: Whether to print intermediate acceptance rates, default True. \n",
    "        :return: AIR.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    eps = 1e-20 # numeric stability and divide by 0 protection\n",
    "    \n",
    "    # reference group summary\n",
    "    reference_accepted = float(cm_dict[reference_key].iat[1,0] + cm_dict[reference_key].iat[1,1]) # predicted 0's\n",
    "    reference_total = float(cm_dict[reference_key].sum().sum())\n",
    "    reference_prop = reference_accepted/reference_total\n",
    "    if verbose:\n",
    "        print(reference_key.title() + ' proportion accepted: %.3f' % reference_prop)\n",
    "    \n",
    "    # protected group summary\n",
    "    protected_accepted = float(cm_dict[protected_key].iat[1,0] + cm_dict[protected_key].iat[1,1]) # predicted 0's\n",
    "    protected_total = float(cm_dict[protected_key].sum().sum())\n",
    "    protected_prop = protected_accepted/protected_total\n",
    "    if verbose:\n",
    "        print(protected_key.title() + ' proportion accepted: %.3f' % protected_prop)\n",
    "\n",
    "    # return adverse impact ratio\n",
    "    return ((protected_prop + eps)/(reference_prop + eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f27f66",
   "metadata": {},
   "source": [
    "### Select Probability Cutoff by F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3105ab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_f1_frame(frame, y, yhat, res=0.01, air_reference=None, air_protected=None): \n",
    "    \n",
    "    \"\"\" Utility function for finding max. F1. \n",
    "        Coupled to get_confusion_matrix() and air(). \n",
    "        Assumes 1 is the marker for class membership.\n",
    "    \n",
    "        :param frame: Pandas dataframe of actual (y) and predicted (yhat) values.\n",
    "        :param y: Known y values.\n",
    "        :param yhat: Model scores.\n",
    "        :param res: Resolution over which to search for max. F1, default 0.01.\n",
    "        :param air_reference: Reference group for AIR calculation, optional.\n",
    "        :param air_protected: Protected group for AIR calculation, optional.\n",
    "        :return: Pandas DataFrame of cutoffs to select from.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    do_air = all(v is not None for v in [air_reference, air_protected])\n",
    "    \n",
    "    # init frame to store f1 at different cutoffs\n",
    "    if do_air:\n",
    "        columns = ['cut', 'f1', 'acc', 'air']\n",
    "    else:\n",
    "        columns = ['cut', 'f1', 'acc']\n",
    "    f1_frame = pd.DataFrame(columns=['cut', 'f1', 'acc'])\n",
    "    \n",
    "    # copy known y and score values into a temporary frame\n",
    "    temp_df = frame[[y, yhat]].copy(deep=True)\n",
    "    \n",
    "    # find f1 at different cutoffs and store in acc_frame\n",
    "    for cut in np.arange(0, 1 + res, res):\n",
    "        temp_df['decision'] = np.where(temp_df.iloc[:, 1] > cut, 1, 0)\n",
    "        f1 = f1_score(temp_df.iloc[:, 0], temp_df['decision'])\n",
    "        acc = accuracy_score(temp_df.iloc[:, 0], temp_df['decision'])\n",
    "        row_dict = {'cut': cut, 'f1': f1, 'acc': acc}\n",
    "        if do_air:\n",
    "            # conditionally calculate AIR  \n",
    "            cm_ref = get_confusion_matrix(frame, y, yhat, by=air_reference, level=1, cutoff=cut, verbose=False)\n",
    "            cm_pro = get_confusion_matrix(frame, y, yhat, by=air_protected, level=1, cutoff=cut, verbose=False)\n",
    "            air_ = air({air_reference: cm_ref, air_protected: cm_pro}, air_reference, air_protected, verbose=False)\n",
    "            row_dict['air'] = air_\n",
    "            \n",
    "        f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
    "            \n",
    "    del temp_df\n",
    "        \n",
    "    return f1_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec67a90",
   "metadata": {},
   "source": [
    "### Find optimal cutoff based on F1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc132436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      cut        f1       acc\n",
      "0    0.00  0.166656  0.090903\n",
      "1    0.01  0.283816  0.587520\n",
      "2    0.02  0.313475  0.661283\n",
      "3    0.03  0.332788  0.702041\n",
      "4    0.04  0.346562  0.728869\n",
      "..    ...       ...       ...\n",
      "96   0.96  0.113098  0.912566\n",
      "97   0.97  0.101147  0.912379\n",
      "98   0.98  0.084402  0.912007\n",
      "99   0.99  0.059897  0.911376\n",
      "100  1.00  0.000000  0.909097\n",
      "\n",
      "[101 rows x 3 columns]\n",
      "\n",
      "Best XGB F1: 0.4062 achieved at cutoff: 0.21 with accuracy: 0.8587.\n"
     ]
    }
   ],
   "source": [
    "f1_frame = get_max_f1_frame(best_mxgb_phat, y_var, 'phat')\n",
    "\n",
    "print(f1_frame)\n",
    "print()\n",
    "\n",
    "max_f1 = f1_frame['f1'].max()\n",
    "best_cut = f1_frame.loc[int(f1_frame['f1'].idxmax()), 'cut'] #idxmax() returns the index of the maximum value\n",
    "acc = f1_frame.loc[int(f1_frame['f1'].idxmax()), 'acc']\n",
    "\n",
    "print('Best XGB F1: %.4f achieved at cutoff: %.2f with accuracy: %.4f.' % (max_f1, best_cut, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e49b400",
   "metadata": {},
   "source": [
    "### Specify Interesting Demographic Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "341f8db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mxgb_phat_copy = best_mxgb_phat.copy()\n",
    "best_mxgb_phat_copy.rename(columns = {\"Gender\":\"male\"}, inplace = True)\n",
    "best_mxgb_phat_copy[\"female\"] = np.where(best_mxgb_phat_copy[\"male\"] == 0, 1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d042ca5",
   "metadata": {},
   "source": [
    "### Confusion Matrix by Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "33aef35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix by male=1\n",
      "             actual: 1 actual: 0\n",
      "predicted: 1      3185      5133\n",
      "predicted: 0      2726     39566\n",
      "\n",
      "Confusion matrix by female=1\n",
      "             actual: 1 actual: 0\n",
      "predicted: 1       187      1755\n",
      "predicted: 0       245     16981\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographic_group_names = ['male', 'female']\n",
    "cm_dict = {}\n",
    "\n",
    "for name in demographic_group_names:\n",
    "    cm_dict[name] = get_confusion_matrix(best_mxgb_phat_copy, y_var, 'phat', by=name, level=1, cutoff=best_cut)\n",
    "    print(cm_dict[name])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca1fa8d",
   "metadata": {},
   "source": [
    "### Find AIR for Female people\n",
    "* protect accepted: female providers\n",
    "* reference accepted: male providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1395cfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male proportion accepted: 0.836\n",
      "Female proportion accepted: 0.899\n",
      "Adverse impact ratio(AIR) for Females vs. Males: 1.075\n"
     ]
    }
   ],
   "source": [
    "print('Adverse impact ratio(AIR) for Females vs. Males: %.3f' % air(cm_dict, 'male', 'female'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d83926d",
   "metadata": {},
   "source": [
    "* Threshold: AIR >= 0.8 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
